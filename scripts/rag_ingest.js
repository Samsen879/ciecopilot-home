// RAG ingestion script: register documents, chunk, embed, and upsert into DB
// Usage:
//   node scripts/rag_ingest.js --subject 9702 --papers AS,A2 --notes --pdf --limit 50 --dry
// Env:
//   VITE_SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY (or VITE_SUPABASE_SERVICE_ROLE_KEY), OPENAI_API_KEY

import dotenv from 'dotenv';
import fs from 'fs';
import path from 'path';
import crypto from 'crypto';
import { fileURLToPath } from 'url';
import { createClient } from '@supabase/supabase-js';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Load env vars from .env first, then .env.local to override if present
dotenv.config();
dotenv.config({ path: path.resolve(__dirname, '../.env.local'), override: true });

// Robust CLI argument parser: supports --key value, --key=value, -k value, and positional args in _
function parseCliArgs(args) {
  const out = {};
  for (let i = 0; i < args.length; i++) {
    const a = args[i];
    if (a.startsWith('--')) {
      const eq = a.indexOf('=');
      if (eq !== -1) {
        const key = a.slice(2, eq);
        const val = a.slice(eq + 1);
        out[key] = val === '' ? true : val;
      } else {
        const key = a.slice(2);
        const next = args[i + 1];
        if (next && !next.startsWith('-')) { out[key] = next; i++; }
        else { out[key] = true; }
      }
    } else if (a.startsWith('-')) {
      const key = a.slice(1);
      const next = args[i + 1];
      if (next && !next.startsWith('-')) { out[key] = next; i++; }
      else { out[key] = true; }
    } else {
      if (!out._) out._ = [];
      out._.push(a);
    }
  }
  return out;
}

// Replace previous naive parser
const argv = parseCliArgs(process.argv.slice(2));
const SUPABASE_URL =
  process.env.VITE_SUPABASE_URL ||
  process.env.SUPABASE_URL;
// Prefer service role for writes with RLS on. Fallback to anon for local/dev if you loosen policies.
const SUPABASE_KEY =
  process.env.SUPABASE_SERVICE_ROLE_KEY ||
  process.env.SUPABASE_SERVICE_ROLE ||
  process.env.VITE_SUPABASE_SERVICE_ROLE_KEY ||
  process.env.SUPABASE_ANON_KEY ||
  process.env.VITE_SUPABASE_ANON_KEY;
// Embedding provider config (supports OpenAI-compatible endpoints like DashScope)
const EMBEDDING_BASE_URL =
  process.env.VECTOR_EMBEDDING_BASE_URL ||
  process.env.EMBEDDING_BASE_URL ||
  process.env.OPENAI_BASE_URL ||
  'https://api.openai.com/v1';
const EMBEDDING_API_KEY =
  process.env.VECTOR_EMBEDDING_API_KEY ||
  process.env.EMBEDDING_API_KEY ||
  process.env.DASHSCOPE_API_KEY ||
  process.env.OPENAI_API_KEY ||
  process.env.OPENAI_API_TOKEN ||
  process.env.OPENAI_KEY;
const EMBEDDING_MODEL =
  process.env.VECTOR_EMBEDDING_MODEL ||
  process.env.EMBEDDING_MODEL ||
  'text-embedding-3-small';
const EMBEDDING_DIMENSIONS = (() => {
  const v = process.env.VECTOR_EMBEDDING_DIMENSIONS || process.env.EMBEDDING_DIMENSIONS
  const n = v ? Number(v) : undefined
  return Number.isFinite(n) && n > 0 ? n : undefined
})()

if (!SUPABASE_URL || !SUPABASE_KEY) {
  throw new Error(
    'Missing Supabase credentials. Provide VITE_SUPABASE_URL (or SUPABASE_URL) and SUPABASE_SERVICE_ROLE_KEY (or SUPABASE_SERVICE_ROLE / VITE_SUPABASE_SERVICE_ROLE_KEY / *ANON_KEY for dev)'
  );
}
if (!EMBEDDING_API_KEY) {
  throw new Error('Missing embedding API key. Provide VECTOR_EMBEDDING_API_KEY (or EMBEDDING_API_KEY / DASHSCOPE_API_KEY / OPENAI_API_KEY).');
}

const supabase = createClient(SUPABASE_URL, SUPABASE_KEY);

// Directories
const ROOT = path.join(__dirname, '..');
const NOTES_DIR = path.join(ROOT, 'src', 'data', 'data-notes');
const PAPERS_DIR = path.join(ROOT, 'data', 'past-papers');
const MARK_SCHEMES_DIR = path.join(ROOT, 'data', 'mark-schemes');

function hashString(s) {
  return crypto.createHash('sha1').update(s).digest('hex');
}

function listFilesRecursive(baseDir, exts) {
  const results = [];
  function walk(dir) {
    const entries = fs.readdirSync(dir, { withFileTypes: true });
    for (const entry of entries) {
      if (entry.name.startsWith('.')) continue;
      const full = path.join(dir, entry.name);
      if (entry.isDirectory()) walk(full);
      else if (exts.includes(path.extname(entry.name).toLowerCase())) results.push(full);
    }
  }
  if (fs.existsSync(baseDir)) walk(baseDir);
  return results;
}

function normalizeSubjectFromPath(p) {
  if (p.includes('9709')) return '9709';
  if (p.includes('9231')) return '9231';
  if (p.includes('9702')) return '9702';
  return 'unknown';
}

function inferPaperCodeFromPath(p) {
  // Notes: may include AS/A2 or paperX in filename
  const lower = p.toLowerCase();
  if (lower.includes('paper1')) return 'paper1';
  if (lower.includes('paper2')) return 'paper2';
  if (lower.includes('paper3')) return 'paper3';
  if (lower.includes('paper4')) return 'paper4';
  if (lower.includes('paper5')) return 'paper5';
  if (lower.includes('paper6')) return 'paper6';
  if (lower.includes('as')) return 'AS';
  if (lower.includes('a2')) return 'A2';
  return null;
}

/**
 * è§£æè¯•å·æ–‡ä»¶åï¼Œæå–è¯¦ç»†çš„å…ƒæ•°æ®ä¿¡æ¯
 * æ”¯æŒæ ¼å¼: 9702_s23_qp_12.pdf, 9702_s23_ms_12.pdf
 * @param {string} filePath - æ–‡ä»¶è·¯å¾„
 * @returns {Object} è§£æåçš„å…ƒæ•°æ®
 */
function parseExamPaperMetadata(filePath) {
  const fileName = path.basename(filePath, '.pdf');
  const isMarkScheme = filePath.toLowerCase().includes('mark-schemes') || fileName.toLowerCase().includes('_ms_');
  const isPastPaper = filePath.toLowerCase().includes('past-papers') || fileName.toLowerCase().includes('_qp_');
  
  // è¯•å·æ–‡ä»¶åæ ¼å¼è§£æ: 9702_s23_qp_12.pdf æˆ– 9702_s23_ms_12.pdf
  const examPattern = /^(\d{4})_([swm])(\d{2})_(qp|ms)_(\d{1,2})$/i;
  const match = fileName.match(examPattern);
  
  if (match) {
    const [, subjectCode, session, year, paperType, paperNumber] = match;
    
    // è§£æè€ƒè¯•å­£
    const sessionMap = {
      's': 'summer',    // å¤å­£
      'w': 'winter',    // å†¬å­£
      'm': 'march'      // ä¸‰æœˆ
    };
    
    // è§£æå¹´ä»½ (23 -> 2023)
    const fullYear = parseInt(year) < 50 ? 2000 + parseInt(year) : 1900 + parseInt(year);
    
    // ç”Ÿæˆpaper_code
    const paperCode = `paper${paperNumber}`;
    
    return {
      subject_code: subjectCode,
      paper_code: paperCode,
      year: fullYear,
      session: sessionMap[session.toLowerCase()] || session,
      paper_number: parseInt(paperNumber),
      paper_type: paperType.toLowerCase(), // 'qp' æˆ– 'ms'
      source_type: isMarkScheme ? 'mark_scheme_pdf' : 'past_paper_pdf',
      is_mark_scheme: isMarkScheme,
      is_past_paper: isPastPaper,
      original_filename: fileName,
      // ç”Ÿæˆå”¯ä¸€çš„paper_idç”¨äºå…³è”è¯•å·å’Œç­”æ¡ˆ
      paper_id: `${subjectCode}_${session}${year}_${paperNumber}`
    };
  }
  
  // å›é€€åˆ°åŸæœ‰é€»è¾‘
  const subject_code = normalizeSubjectFromPath(filePath);
  const paper_code = inferPaperCodeFromPath(filePath);
  
  return {
    subject_code,
    paper_code,
    year: null,
    session: null,
    paper_number: null,
    paper_type: null,
    source_type: isMarkScheme ? 'mark_scheme_pdf' : 'past_paper_pdf',
    is_mark_scheme: isMarkScheme,
    is_past_paper: isPastPaper,
    original_filename: fileName,
    paper_id: null
  };
}

async function extractPdfText(filePath) {
  // Lazily import pdf-parse so that running notes-only ingestion doesn't require PDF test assets
  const pdf = (await import('pdf-parse')).default;
  const data = await pdf(fs.readFileSync(filePath));
  // Simple page split approximation; pdf-parse returns text merged, not per page.
  // As a minimal viable approach, split by form feed or by \n\n\n.
  const pages = data.text.split(/\f|\n\n\n/g).map((t) => t.trim()).filter(Boolean);
  return pages;
}

function chunkTextByTokens(text, targetTokens = 600, overlapTokens = 80) {
  const words = text.split(/\s+/);
  const approxTokens = (w) => Math.ceil(w.length * 0.75);
  const chunks = [];
  let start = 0;
  while (start < words.length) {
    let end = start;
    let tokens = 0;
    while (end < words.length && tokens < targetTokens) {
      tokens += approxTokens(words[end]);
      end += 1;
    }
    const content = words.slice(start, end).join(' ');
    chunks.push(content);
    if (end >= words.length) break;
    start = Math.max(end - Math.ceil(overlapTokens / 1.0), start + 1);
  }
  return chunks;
}

// æ–°å¢ï¼šç®€å•çš„sleepå·¥å…·ï¼Œç”¨äºèŠ‚æµå’Œé€€é¿ç­‰å¾…
function sleep(ms) {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

async function embedBatch(texts) {
  // Determine provider-specific batch limits. DashScope/Qwen supports up to 10 items per request.
  const lowerBase = (EMBEDDING_BASE_URL || '').toLowerCase();
  const lowerModel = (EMBEDDING_MODEL || '').toLowerCase();
  const envBatch = Number(process.env.VECTOR_EMBEDDING_BATCH_SIZE || process.env.EMBEDDING_BATCH_SIZE);
  const maxBatchSize = Number.isFinite(envBatch) && envBatch > 0
    ? envBatch
    : (lowerBase.includes('dashscope') || lowerModel.includes('qwen') || lowerModel.includes('text-embedding-v4'))
      ? 10
      : 2048; // OpenAI-compatible default

  // èŠ‚æµä¸é‡è¯•é…ç½®ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
  const throttleMsRaw = process.env.VECTOR_EMBEDDING_THROTTLE_MS || process.env.EMBEDDING_THROTTLE_MS;
  const throttleMs = Number.isFinite(Number(throttleMsRaw)) ? Number(throttleMsRaw)
    : (lowerBase.includes('dashscope') || lowerModel.includes('qwen') || lowerModel.includes('text-embedding-v4'))
      ? 400
      : 0; // é»˜è®¤å¯¹DashScopeåšé€‚åº¦èŠ‚æµ
  const maxRetriesRaw = process.env.VECTOR_EMBEDDING_MAX_RETRIES || process.env.EMBEDDING_MAX_RETRIES;
  const maxRetries = Number.isFinite(Number(maxRetriesRaw)) && Number(maxRetriesRaw) >= 0 ? Number(maxRetriesRaw) : 4;
  const baseDelayRaw = process.env.VECTOR_EMBEDDING_RETRY_BASE_MS || process.env.EMBEDDING_RETRY_BASE_MS;
  const baseDelayMs = Number.isFinite(Number(baseDelayRaw)) && Number(baseDelayRaw) > 0 ? Number(baseDelayRaw) : 500;

  const shouldRetry = (status, err) => {
    if (!status) return true; // ç½‘ç»œé”™è¯¯
    if ([429, 502, 503, 504].includes(status)) return true; // å¸¸è§é™æµ/ç½‘å…³é”™è¯¯
    if (status >= 500 && status < 600) return true; // å…¶ä»–5xx
    return false;
  };

  const results = [];
  for (let start = 0; start < texts.length; start += maxBatchSize) {
    const slice = texts.slice(start, start + maxBatchSize);

    let attempt = 0;
    while (true) {
      try {
        const resp = await fetch(`${EMBEDDING_BASE_URL.replace(/\/$/, '')}/embeddings`, {
          method: 'POST',
          headers: { Authorization: `Bearer ${EMBEDDING_API_KEY}`, 'Content-Type': 'application/json' },
          body: JSON.stringify({ model: EMBEDDING_MODEL, input: slice, ...(EMBEDDING_DIMENSIONS ? { dimensions: EMBEDDING_DIMENSIONS } : {}) }),
        });

        if (!resp.ok) {
          let msg = `Embedding request failed with status ${resp.status}`;
          let detail = '';
          try { const e = await resp.json(); detail = e?.error?.message || e?.message || ''; } catch {}
          if (shouldRetry(resp.status)) {
            attempt += 1;
            if (attempt > maxRetries) {
              throw new Error(`${msg}${detail ? `: ${detail}` : ''} (exceeded retries)`);
            }
            const backoff = baseDelayMs * 2 ** (attempt - 1) + Math.random() * 120;
            await sleep(backoff);
            continue; // é‡è¯•
          }
          // ä¸å¯é‡è¯•çš„é”™è¯¯
          throw new Error(detail ? `${msg}: ${detail}` : msg);
        }

        const data = await resp.json();
        const arr = Array.isArray(data?.data) ? data.data : [];
        const embeddings = arr.map((d) => d.embedding);
        if (embeddings.length !== slice.length) {
          throw new Error(`Embedding response count ${embeddings.length} does not match request count ${slice.length}`);
        }
        results.push(...embeddings);
        break; // æˆåŠŸï¼Œé€€å‡ºé‡è¯•å¾ªç¯
      } catch (err) {
        // ç½‘ç»œçº§é”™è¯¯ï¼ˆfetchå¤±è´¥ç­‰ï¼‰
        attempt += 1;
        if (attempt > maxRetries) {
          throw new Error(`Embedding request failed after ${maxRetries} retries: ${err?.message || err}`);
        }
        const backoff = baseDelayMs * 2 ** (attempt - 1) + Math.random() * 120;
        await sleep(backoff);
      }
    }

    if (throttleMs > 0 && (start + maxBatchSize) < texts.length) {
      await sleep(throttleMs);
    }
  }
  return results;
}

async function upsertDocument({ subject_code, paper_code, topic_id, title, source_type, source_path, language = 'zh', tags = [] }) {
  const { data, error } = await supabase
    .from('rag_documents')
    .upsert({ subject_code, paper_code, topic_id, title, source_type, source_path, language, tags }, { onConflict: 'subject_code,paper_code,source_type,source_path' })
    .select()
    .single();
  if (error) throw error;
  return data;
}

async function upsertChunk({ document_id, chunk_index, content, token_count = null, page_from = null, page_to = null, extra = {} }) {
  const { data, error } = await supabase
    .from('rag_chunks')
    .upsert({ document_id, chunk_index, content, token_count, page_from, page_to, extra }, { onConflict: 'document_id,chunk_index' })
    .select()
    .single();
  if (error) throw error;
  return data;
}

async function upsertEmbedding({ chunk_id, embedding }) {
  const { data, error } = await supabase
    .from('rag_embeddings')
    .upsert({ chunk_id, embedding }, { onConflict: 'chunk_id' })
    .select()
    .single();
  if (error) throw error;
  return data;
}

async function ingestMarkdownNotes({ subjectFilter, paperFilter, limit = Infinity, dryRun = false }) {
  const noteFiles = listFilesRecursive(NOTES_DIR, ['.md']).filter((p) => /9702|9709|9231/.test(p));
  console.log(`Found ${noteFiles.length} markdown files with subject pattern`);
  
  let count = 0;
  for (const file of noteFiles) {
    if (count >= limit) break;
    const subject_code = normalizeSubjectFromPath(file);
    console.log(`File: ${file}, Subject: ${subject_code}, Filter: ${subjectFilter}`);
    
    if (subjectFilter && subject_code !== subjectFilter) {
      console.log(`  Skipping: subject mismatch (${subject_code} !== ${subjectFilter})`);
      continue;
    }
    
    const paper_code = inferPaperCodeFromPath(file);
    if (paperFilter && paper_code !== paperFilter) {
      console.log(`  Skipping: paper mismatch (${paper_code} !== ${paperFilter})`);
      continue;
    }

    const title = path.basename(file, path.extname(file)).replace(/[_-]+/g, ' ');
    const source_type = 'note_md';
    const source_path = path.relative(ROOT, file).replace(/\\/g, '/');
    const topic_id = `${subject_code}-${hashString(source_path).slice(0, 8)}`;
    const content = fs.readFileSync(file, 'utf8');

    const chunks = chunkTextByTokens(content, 600, 100);
    if (chunks.length === 0) {
      console.log(`  Skipping: no content chunks generated`);
      continue;
    }

    console.log(`[notes] ${file} -> ${chunks.length} chunks`);
    if (dryRun) { count += 1; continue; }

    try {
      console.log(`  Upserting document...`);
      const doc = await upsertDocument({ subject_code, paper_code, topic_id, title, source_type, source_path, language: 'zh' });
      console.log(`  Document upserted: ${doc.id}`);
      
      console.log(`  Generating embeddings for ${chunks.length} chunks...`);
      const embeddings = await embedBatch(chunks);
      console.log(`  Embeddings generated: ${embeddings.length}`);
      
      for (let i = 0; i < chunks.length; i++) {
        console.log(`  Upserting chunk ${i + 1}/${chunks.length}...`);
        const chunk = await upsertChunk({ document_id: doc.id, chunk_index: i, content: chunks[i], token_count: null, extra: { kind: 'md' } });
        console.log(`  Chunk upserted: ${chunk.id}`);
        
        console.log(`  Upserting embedding for chunk ${chunk.id}...`);
        await upsertEmbedding({ chunk_id: chunk.id, embedding: embeddings[i] });
        console.log(`  Embedding upserted for chunk ${chunk.id}`);
      }
      console.log(`  âœ… Successfully processed file: ${file}`);
    } catch (error) {
      console.error(`  âŒ Error processing file ${file}:`, error.message);
      throw error;
    }
    count += 1;
  }
  console.log(`Processed ${count} files total`);
}

async function ingestPdfs({ subjectFilter, paperFilter, limit = 30, dryRun = false }) {
  const pdfDirs = [PAPERS_DIR, MARK_SCHEMES_DIR];
  let count = 0;
  let processed = 0;
  let skipped = 0;
  let errors = 0;
  
  console.log(`\nğŸš€ å¼€å§‹PDFæ‰¹é‡å¤„ç†...`);
  console.log(`ğŸ“ æ‰«æç›®å½•: ${pdfDirs.map(d => path.relative(ROOT, d)).join(', ')}`);
  console.log(`ğŸ¯ ç­›é€‰æ¡ä»¶: ç§‘ç›®=${subjectFilter || 'å…¨éƒ¨'}, è¯•å·=${paperFilter || 'å…¨éƒ¨'}`);
  console.log(`ğŸ“Š å¤„ç†é™åˆ¶: ${limit} ä¸ªæ–‡ä»¶`);
  console.log(`ğŸ” æ¨¡å¼: ${dryRun ? 'DRY RUN (ä»…é¢„è§ˆ)' : 'å®é™…å¤„ç†'}\n`);
  
  for (const base of pdfDirs) {
    const files = listFilesRecursive(base, ['.pdf']);
    console.log(`ğŸ“‚ åœ¨ ${path.relative(ROOT, base)} ä¸­æ‰¾åˆ° ${files.length} ä¸ªPDFæ–‡ä»¶`);
    
    for (const file of files) {
      if (count >= limit) break;
      
      try {
        // ä½¿ç”¨æ–°çš„å…ƒæ•°æ®è§£æå‡½æ•°
        const metadata = parseExamPaperMetadata(file);
        const { subject_code, paper_code, source_type, year, session, paper_number, paper_type, paper_id, original_filename } = metadata;
        
        // åº”ç”¨ç­›é€‰æ¡ä»¶
        if (subjectFilter && subject_code !== subjectFilter) {
          skipped++;
          continue;
        }
        if (paperFilter && paper_code !== paperFilter) {
          skipped++;
          continue;
        }

        // ç”Ÿæˆå¢å¼ºçš„æ ‡é¢˜å’Œæ ‡ç­¾
        const title = year && session && paper_number 
          ? `${subject_code} ${year} ${session} Paper ${paper_number} ${paper_type === 'ms' ? '(Mark Scheme)' : '(Question Paper)'}`
          : original_filename;
        
        const tags = [
          subject_code,
          paper_code,
          year ? year.toString() : null,
          session,
          paper_type,
          source_type
        ].filter(Boolean);
        
        const source_path = path.relative(ROOT, file).replace(/\\/g, '/');
        const topic_id = null;

        console.log(`\nğŸ“„ å¤„ç†æ–‡ä»¶ [${count + 1}/${limit}]: ${path.basename(file)}`);
        console.log(`   ğŸ“‹ å…ƒæ•°æ®: ${subject_code} | ${year || 'N/A'} ${session || 'N/A'} | Paper ${paper_number || 'N/A'} | ${paper_type || 'N/A'}`);
        console.log(`   ğŸ·ï¸  ç±»å‹: ${source_type}`);
        console.log(`   ğŸ†” Paper ID: ${paper_id || 'N/A'}`);

        const pages = await extractPdfText(file);
        const chunks = pages.flatMap((pageText, idx) => {
          const sub = chunkTextByTokens(pageText, 500, 60);
          return sub.map((txt) => ({ txt, page: idx + 1 }));
        });
        
        if (chunks.length === 0) {
          console.log(`   âš ï¸  è­¦å‘Š: æ— æ³•æå–æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡`);
          skipped++;
          continue;
        }

        console.log(`   ğŸ“ æå–å†…å®¹: ${pages.length} é¡µ -> ${chunks.length} ä¸ªæ–‡æœ¬å—`);
        
        if (dryRun) { 
          console.log(`   âœ… DRY RUN: æ¨¡æ‹Ÿå¤„ç†å®Œæˆ`);
          count += 1; 
          continue; 
        }

        // åˆ›å»ºæ–‡æ¡£è®°å½•ï¼ŒåŒ…å«å¢å¼ºçš„å…ƒæ•°æ®
         const docData = {
           subject_code,
           paper_code,
           topic_id,
           title,
           source_type,
           source_path,
           language: 'en',
           tags
         };
        
        const doc = await upsertDocument(docData);
        console.log(`   ğŸ’¾ æ–‡æ¡£å·²ä¿å­˜: ID=${doc.id}`);
        
        // æ‰¹é‡å¤„ç†åµŒå…¥
        console.log(`   ğŸ”„ ç”Ÿæˆå‘é‡åµŒå…¥...`);
        const embeddings = await embedBatch(chunks.map((c) => c.txt));
        
        console.log(`   ğŸ“¤ ä¸Šä¼ æ–‡æœ¬å—å’ŒåµŒå…¥...`);
        for (let i = 0; i < chunks.length; i++) {
          const { txt, page } = chunks[i];
          const chunkExtra = {
             kind: 'pdf',
             year,
             session,
             paper_number,
             paper_type,
             paper_id,
             original_filename
           };
          const chunk = await upsertChunk({ 
            document_id: doc.id, 
            chunk_index: i, 
            content: txt, 
            page_from: page, 
            page_to: page, 
            extra: chunkExtra 
          });
          await upsertEmbedding({ chunk_id: chunk.id, embedding: embeddings[i] });
        }
        
        processed++;
        count += 1;
        console.log(`   âœ… å¤„ç†å®Œæˆ: ${chunks.length} ä¸ªå—å·²ä¿å­˜`);
        
      } catch (error) {
        console.error(`   âŒ å¤„ç†å¤±è´¥: ${error.message}`);
        errors++;
        // ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶
      }
    }
  }
  
  // è¾“å‡ºå¤„ç†ç»Ÿè®¡
  console.log(`\nğŸ“Š å¤„ç†ç»Ÿè®¡:`);
  console.log(`   âœ… æˆåŠŸå¤„ç†: ${processed} ä¸ªæ–‡ä»¶`);
  console.log(`   â­ï¸  è·³è¿‡æ–‡ä»¶: ${skipped} ä¸ªæ–‡ä»¶`);
  console.log(`   âŒ å¤„ç†å¤±è´¥: ${errors} ä¸ªæ–‡ä»¶`);
  console.log(`   ğŸ“ˆ æ€»è®¡æ‰«æ: ${processed + skipped + errors} ä¸ªæ–‡ä»¶`);
  
  if (dryRun) {
    console.log(`\nğŸ” DRY RUN æ¨¡å¼å®Œæˆï¼Œå®é™…æœªå†™å…¥æ•°æ®åº“`);
  } else {
    console.log(`\nğŸ‰ PDFæ‰¹é‡å¤„ç†å®Œæˆï¼`);
  }
}

/**
 * æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨äºæ•°æ®åº“ä¸­
 * @param {Object} metadata - æ–‡æ¡£å…ƒæ•°æ®
 * @returns {Promise<boolean>} æ˜¯å¦å·²å­˜åœ¨
 */
async function checkDocumentExists({ subject_code, paper_code, source_type, source_path }) {
  const { data, error } = await supabase
    .from('rag_documents')
    .select('id')
    .eq('subject_code', subject_code)
    .eq('paper_code', paper_code || '')
    .eq('source_type', source_type)
    .eq('source_path', source_path)
    .single();
  
  if (error && error.code !== 'PGRST116') { // PGRST116 = no rows returned
    console.warn(`æ£€æŸ¥æ–‡æ¡£å­˜åœ¨æ€§æ—¶å‡ºé”™: ${error.message}`);
    return false;
  }
  
  return !!data;
}

/**
 * å¢å¼ºçš„PDFæ‰¹é‡å¤„ç†å‡½æ•°ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œé”™è¯¯æ¢å¤
 */
async function ingestPdfsEnhanced({ subjectFilter, paperFilter, limit = 30, dryRun = false, skipExisting = true, continueOnError = true }) {
  const pdfDirs = [PAPERS_DIR, MARK_SCHEMES_DIR];
  let count = 0;
  let processed = 0;
  let skipped = 0;
  let errors = 0;
  let existingSkipped = 0;
  
  console.log(`\nğŸš€ å¼€å§‹å¢å¼ºPDFæ‰¹é‡å¤„ç†...`);
  console.log(`ğŸ“ æ‰«æç›®å½•: ${pdfDirs.map(d => path.relative(ROOT, d)).join(', ')}`);
  console.log(`ğŸ¯ ç­›é€‰æ¡ä»¶: ç§‘ç›®=${subjectFilter || 'å…¨éƒ¨'}, è¯•å·=${paperFilter || 'å…¨éƒ¨'}`);
  console.log(`ğŸ“Š å¤„ç†é™åˆ¶: ${limit} ä¸ªæ–‡ä»¶`);
  console.log(`ğŸ” æ¨¡å¼: ${dryRun ? 'DRY RUN (ä»…é¢„è§ˆ)' : 'å®é™…å¤„ç†'}`);
  console.log(`â­ï¸  è·³è¿‡å·²å­˜åœ¨: ${skipExisting ? 'æ˜¯' : 'å¦'}`);
  console.log(`ğŸ”„ é”™è¯¯ç»§ç»­: ${continueOnError ? 'æ˜¯' : 'å¦'}\n`);
  
  // æ”¶é›†æ‰€æœ‰PDFæ–‡ä»¶
  const allFiles = [];
  for (const base of pdfDirs) {
    const files = listFilesRecursive(base, ['.pdf']);
    allFiles.push(...files.map(file => ({ file, base })));
  }
  
  console.log(`ğŸ“‚ æ€»å…±æ‰¾åˆ° ${allFiles.length} ä¸ªPDFæ–‡ä»¶`);
  
  for (const { file, base } of allFiles) {
    if (count >= limit) {
      console.log(`\nâ¹ï¸  è¾¾åˆ°å¤„ç†é™åˆ¶ (${limit})ï¼Œåœæ­¢å¤„ç†`);
      break;
    }
    
    try {
      // ä½¿ç”¨æ–°çš„å…ƒæ•°æ®è§£æå‡½æ•°
      const metadata = parseExamPaperMetadata(file);
      const { subject_code, paper_code, source_type, year, session, paper_number, paper_type, paper_id, original_filename } = metadata;
      
      // åº”ç”¨ç­›é€‰æ¡ä»¶
      if (subjectFilter && subject_code !== subjectFilter) {
        skipped++;
        continue;
      }
      if (paperFilter && paper_code !== paperFilter) {
        skipped++;
        continue;
      }

      // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆå¦‚æœå¯ç”¨è·³è¿‡å·²å­˜åœ¨ï¼‰
      if (skipExisting && !dryRun) {
        const exists = await checkDocumentExists({
           subject_code,
           paper_code,
           source_type,
           source_path: path.relative(ROOT, file).replace(/\\/g, '/')
         });
        
        if (exists) {
          console.log(`\nâ­ï¸  è·³è¿‡å·²å­˜åœ¨ [${count + 1}]: ${path.basename(file)}`);
          existingSkipped++;
          count++;
          continue;
        }
      }

      // ç”Ÿæˆå¢å¼ºçš„æ ‡é¢˜å’Œæ ‡ç­¾
      const title = year && session && paper_number 
        ? `${subject_code} ${year} ${session} Paper ${paper_number} ${paper_type === 'ms' ? '(Mark Scheme)' : '(Question Paper)'}`
        : original_filename;
      
      const tags = [
        subject_code,
        paper_code,
        year ? year.toString() : null,
        session,
        paper_type,
        source_type
      ].filter(Boolean);
      
      const source_path = path.relative(ROOT, file).replace(/\\/g, '/');
      const topic_id = null;

      console.log(`\nğŸ“„ å¤„ç†æ–‡ä»¶ [${count + 1}/${limit}]: ${path.basename(file)}`);
      console.log(`   ğŸ“‹ å…ƒæ•°æ®: ${subject_code} | ${year || 'N/A'} ${session || 'N/A'} | Paper ${paper_number || 'N/A'} | ${paper_type || 'N/A'}`);
      console.log(`   ğŸ·ï¸  ç±»å‹: ${source_type}`);
      console.log(`   ğŸ†” Paper ID: ${paper_id || 'N/A'}`);

      const pages = await extractPdfText(file);
      const chunks = pages.flatMap((pageText, idx) => {
        const sub = chunkTextByTokens(pageText, 500, 60);
        return sub.map((txt) => ({ txt, page: idx + 1 }));
      });
      
      if (chunks.length === 0) {
        console.log(`   âš ï¸  è­¦å‘Š: æ— æ³•æå–æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡`);
        skipped++;
        count++;
        continue;
      }

      console.log(`   ğŸ“ æå–å†…å®¹: ${pages.length} é¡µ -> ${chunks.length} ä¸ªæ–‡æœ¬å—`);
      
      if (dryRun) { 
        console.log(`   âœ… DRY RUN: æ¨¡æ‹Ÿå¤„ç†å®Œæˆ`);
        count += 1; 
        continue; 
      }

      // åˆ›å»ºæ–‡æ¡£è®°å½•ï¼ŒåŒ…å«å¢å¼ºçš„å…ƒæ•°æ®
      const docData = {
        subject_code,
        paper_code,
        topic_id,
        title,
        source_type,
        source_path,
        language: 'en',
        tags
      };
      
      const doc = await upsertDocument(docData);
      console.log(`   ğŸ’¾ æ–‡æ¡£å·²ä¿å­˜: ID=${doc.id}`);
      
      // æ‰¹é‡å¤„ç†åµŒå…¥
      console.log(`   ğŸ”„ ç”Ÿæˆå‘é‡åµŒå…¥...`);
      const embeddings = await embedBatch(chunks.map((c) => c.txt));
      
      console.log(`   ğŸ“¤ ä¸Šä¼ æ–‡æœ¬å—å’ŒåµŒå…¥...`);
      for (let i = 0; i < chunks.length; i++) {
        const { txt, page } = chunks[i];
        const chunkExtra = {
          kind: 'pdf',
          year,
          session,
          paper_number,
          paper_type,
          paper_id,
          original_filename
        };
        const chunk = await upsertChunk({ 
          document_id: doc.id, 
          chunk_index: i, 
          content: txt, 
          page_from: page, 
          page_to: page, 
          extra: chunkExtra 
        });
        await upsertEmbedding({ chunk_id: chunk.id, embedding: embeddings[i] });
      }
      
      processed++;
      count += 1;
      console.log(`   âœ… å¤„ç†å®Œæˆ: ${chunks.length} ä¸ªå—å·²ä¿å­˜`);
      
    } catch (error) {
      console.error(`   âŒ å¤„ç†å¤±è´¥: ${error.message}`);
      if (error.stack) {
        console.error(`   ğŸ“ é”™è¯¯å †æ ˆ: ${error.stack.split('\n')[1]}`);
      }
      errors++;
      count++;
      
      if (!continueOnError) {
        console.error(`\nğŸ›‘ é‡åˆ°é”™è¯¯ä¸”æœªå¯ç”¨é”™è¯¯ç»§ç»­æ¨¡å¼ï¼Œåœæ­¢å¤„ç†`);
        break;
      }
      
      // ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶
      console.log(`   ğŸ”„ ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶...`);
    }
  }
  
  // è¾“å‡ºå¤„ç†ç»Ÿè®¡
  console.log(`\nğŸ“Š å¤„ç†ç»Ÿè®¡:`);
  console.log(`   âœ… æˆåŠŸå¤„ç†: ${processed} ä¸ªæ–‡ä»¶`);
  console.log(`   â­ï¸  ç­›é€‰è·³è¿‡: ${skipped} ä¸ªæ–‡ä»¶`);
  console.log(`   ğŸ”„ å·²å­˜åœ¨è·³è¿‡: ${existingSkipped} ä¸ªæ–‡ä»¶`);
  console.log(`   âŒ å¤„ç†å¤±è´¥: ${errors} ä¸ªæ–‡ä»¶`);
  console.log(`   ğŸ“ˆ æ€»è®¡æ‰«æ: ${allFiles.length} ä¸ªæ–‡ä»¶`);
  console.log(`   ğŸ“Š å®é™…å¤„ç†: ${count} ä¸ªæ–‡ä»¶`);
  
  if (dryRun) {
    console.log(`\nğŸ” DRY RUN æ¨¡å¼å®Œæˆï¼Œå®é™…æœªå†™å…¥æ•°æ®åº“`);
  } else {
    console.log(`\nğŸ‰ å¢å¼ºPDFæ‰¹é‡å¤„ç†å®Œæˆï¼`);
  }
  
  return {
    processed,
    skipped,
    existingSkipped,
    errors,
    total: allFiles.length
  };
}

async function main() {
  const subject = argv.subject || argv.s;
  const papers = (argv.papers || '').split(',').filter(Boolean);
  const doNotes = argv.notes || false;
  const doPdf = argv.pdf || false;
  const limit = argv.limit ? parseInt(argv.limit, 10) : undefined;
  const dry = !!argv.dry;
  const skipExisting = argv['skip-existing'] !== false; // é»˜è®¤ä¸ºtrue
  const continueOnError = argv['continue-on-error'] !== false; // é»˜è®¤ä¸ºtrue

  if (!doNotes && !doPdf) {
    console.log('Nothing to ingest. Use --notes and/or --pdf');
    console.log('\nå¯ç”¨é€‰é¡¹:');
    console.log('  --subject 9702        æŒ‡å®šç§‘ç›®ä»£ç ');
    console.log('  --papers AS,A2        æŒ‡å®šè¯•å·ç±»å‹');
    console.log('  --limit 50            é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡');
    console.log('  --dry                 å¹²è¿è¡Œæ¨¡å¼ï¼ˆä»…é¢„è§ˆï¼‰');
    console.log('  --skip-existing       è·³è¿‡å·²å­˜åœ¨çš„æ–‡æ¡£ï¼ˆé»˜è®¤å¯ç”¨ï¼‰');
    console.log('  --continue-on-error   é‡åˆ°é”™è¯¯æ—¶ç»§ç»­å¤„ç†ï¼ˆé»˜è®¤å¯ç”¨ï¼‰');
    console.log('  --notes               å¤„ç†Markdownç¬”è®°');
    console.log('  --pdf                 å¤„ç†PDFæ–‡ä»¶');
    process.exit(0);
  }

  if (doNotes) {
    console.log('ğŸ”„ å¼€å§‹å¤„ç†Markdownç¬”è®°...');
    await ingestMarkdownNotes({ subjectFilter: subject, paperFilter: papers[0], limit: limit || Infinity, dryRun: dry });
  }
  
  if (doPdf) {
    console.log('ğŸ”„ å¼€å§‹å¤„ç†PDFæ–‡ä»¶...');
    const result = await ingestPdfsEnhanced({ 
      subjectFilter: subject, 
      paperFilter: papers[0], 
      limit: limit || 30, 
      dryRun: dry,
      skipExisting,
      continueOnError
    });
    
    console.log(`\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡: æˆåŠŸ${result.processed}, è·³è¿‡${result.skipped + result.existingSkipped}, å¤±è´¥${result.errors}`);
  }

  console.log('\nğŸ‰ æ•°æ®æ‘„å–å®Œæˆï¼');
}

main().catch((err) => {
  console.error('Ingestion failed:', err);
  process.exit(1);
});


